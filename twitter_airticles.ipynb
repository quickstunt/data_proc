{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UjOxivrBsp0gZ9jwwhNl3OdrbchGyjA3",
      "authorship_tag": "ABX9TyMu1iboQ9I4J8XNud0AynpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quickstunt/data_proc/blob/main/twitter_airticles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAfCEpAnsJCl"
      },
      "outputs": [],
      "source": [
        "#当日公開分のみ用\n",
        "import pandas as pd\n",
        "\n",
        "#データの読み込み\n",
        "#PVのCSVファイル\n",
        "input_csv_file = \"/content/20220816-20220816.csv\"\n",
        "#公開記事が一覧してあるCSVファイル\n",
        "input_article_file = \"/content/8_16.csv\"\n",
        "#投稿した記事ID\n",
        "targetID_path = \"https://URL/article/ArticleID/\"\n",
        "\n",
        "#データの変換\n",
        "data_original = pd.read_csv(input_csv_file)\n",
        "data_articles = pd.read_csv(input_article_file)\n",
        "test_date = input_csv_file[9:26]\n",
        "targetID = targetID_path[32:40]\n",
        "\n",
        "#「時間帯」「記事ID」「ページビュー数」の取り出し\n",
        "data_master = data_original[[\"時間帯\", \"記事ID\", \"ページビュー数\"]]\n",
        "\n",
        "#「時間帯」を文字列に変換\n",
        "data_master[\"時間帯\"] = data_master[\"時間帯\"].astype(str)\n",
        "\n",
        "#各項目で欠損値の有無を確認\n",
        "data_master.isnull().any(axis = 0)\n",
        "\n",
        "#data_masterを読み込んで、記事IDの形式を揃え、投稿記事IDを取り除く\n",
        "data_master[\"記事ID\"] = data_master[\"記事ID\"].str[:18]\n",
        "data_targetID_locs = data_master[\"記事ID\"].str.contains(targetID)\n",
        "data_nontargetID = data_master.loc[~data_targetID_locs]\n",
        "\n",
        "#当日記事IDの形式を整える\n",
        "data_articles[\"記事ID\"] = data_articles[\"記事ID\"].str.replace(\"https://URL/article/\", \"\")\n",
        "data_articles[\"記事ID\"] = data_articles[\"記事ID\"].str.replace(\"/\", \"\")\n",
        "\n",
        "#当日記事IDから投稿記事IDを取り除く\n",
        "data_targetID_articles_locs = data_articles[\"記事ID\"].str.contains(targetID)\n",
        "data_articles[\"記事ID\"] = data_articles.loc[~data_targetID_articles_locs]\n",
        "data_articles.dropna(inplace=True)\n",
        "\n",
        "#当日記事IDから投稿記事IDを取り除いたものをarticleIDsとする\n",
        "articleIDs = data_articles[\"記事ID\"]\n",
        "\n",
        "#当日記事IDのデータを保存するデータフレームの定義\n",
        "data_articleIDs_temp = pd.DataFrame(index = [\"時間帯\", \"記事ID\", \"ページビュー数\"], columns = [])\n",
        "data_articleIDs = pd.DataFrame(index = [\"時間帯\", \"記事ID\", \"ページビュー数\"], columns = [])\n",
        "\n",
        "#当日記事IDのデータをデータフレームに格納\n",
        "for articleID in articleIDs:\n",
        "  data_articleID_locs = data_nontargetID[\"記事ID\"].str.contains(articleID)\n",
        "  data_articleIDs_temp = data_nontargetID.loc[data_articleID_locs]\n",
        "  data_articleIDs = pd.concat([data_articleIDs, data_articleIDs_temp], ignore_index = True)\n",
        "\n",
        "data_articleIDs.dropna(inplace=True)\n",
        "\n",
        "#当日記事IDの投稿数を計算\n",
        "pages_per_time2 = data_articleIDs.groupby(\"時間帯\").nunique()[\"記事ID\"]\n",
        "pv_per_time2 = data_articleIDs.groupby(\"時間帯\").sum()[\"ページビュー数\"]\n",
        "pv_mean_by_time2 = pv_per_time2 / pages_per_time2\n",
        "\n",
        "#pv_mean_by_time2をデータフレーム化\n",
        "data_articleIDs_output = pd.DataFrame(pv_mean_by_time2)\n",
        "\n",
        "#data_output.columnsの列名として「投稿以外平均ページビュー」を付与\n",
        "data_articleIDs_output.columns = ['投稿以外平均ページビュー']\n",
        "\n",
        "\n",
        "#読まれてます投稿分のページビュー\n",
        "\n",
        "#data_masterを読み込んで、記事IDの形式を揃え、投稿記事IDのみを取り出す\n",
        "data_master[\"記事ID\"] = data_master[\"記事ID\"].str[:18]\n",
        "data_targetID_locs = data_master[\"記事ID\"].str.contains(targetID)\n",
        "data_targetID_master = data_master.loc[data_targetID_locs]\n",
        "\n",
        "#出力項目を「時間帯」「ページビュー数」にした出力用データフレームを定義\n",
        "data_targetID_output = data_targetID_master[[\"時間帯\", \"ページビュー数\"]]\n",
        "\n",
        "#data_articleIDs_outputとdata_targetID_outputをマージ\n",
        "data_output = pd.merge(data_articleIDs_output, data_targetID_output, on = \"時間帯\", how='outer')\n",
        "\n",
        "#結合時に時間帯の並びが変わる可能性があるので、時間帯でソート\n",
        "data_output = data_output.sort_values(by = [\"時間帯\"])\n",
        "\n",
        "#データフレームにビューがない時間帯を付け加える\n",
        "import numpy as np\n",
        "\n",
        "#年月日をファイル名から抽出\n",
        "test_date_single = input_csv_file[9:17]\n",
        "\n",
        "#24時間帯の仮データ（0）を持つダミーのデータフレームを作成\n",
        "data_dummy_24 = pd.DataFrame(np.array([[test_date_single + \"00\",0],\n",
        "                         [test_date_single + \"01\",0],\n",
        "                         [test_date_single + \"02\",0],\n",
        "                         [test_date_single + \"03\",0],\n",
        "                         [test_date_single + \"04\",0],\n",
        "                         [test_date_single + \"05\",0],\n",
        "                         [test_date_single + \"06\",0],\n",
        "                         [test_date_single + \"07\",0],\n",
        "                         [test_date_single + \"08\",0],\n",
        "                         [test_date_single + \"09\",0],\n",
        "                         [test_date_single + \"10\",0],\n",
        "                         [test_date_single + \"11\",0],\n",
        "                         [test_date_single + \"12\",0],\n",
        "                         [test_date_single + \"13\",0],\n",
        "                         [test_date_single + \"14\",0],\n",
        "                         [test_date_single + \"15\",0],\n",
        "                         [test_date_single + \"16\",0],\n",
        "                         [test_date_single + \"17\",0],\n",
        "                         [test_date_single + \"18\",0],\n",
        "                         [test_date_single + \"19\",0],\n",
        "                         [test_date_single + \"20\",0],\n",
        "                         [test_date_single + \"21\",0],\n",
        "                         [test_date_single + \"22\",0],\n",
        "                         [test_date_single + \"23\",0],\n",
        "                         ]),\n",
        "                        columns=['時間帯', 'ページビュー数ダミー'])\n",
        "\n",
        "#仮のデータフレームと出力結果をマージ\n",
        "data_merged_24 = pd.merge(data_dummy_24, data_output, on = \"時間帯\", how = \"outer\")\n",
        "#「時間帯」「投稿以外平均ページビュー」「ページビュー数」だけを出力結果とし、欠損値は0とする\n",
        "data_out_24 = data_merged_24.fillna(\"0\")[[\"時間帯\", \"投稿以外平均ページビュー\", \"ページビュー数\"]]\n",
        "\n",
        "#欠損値を0にしてCSVファイルに出力\n",
        "output_csv_file5 = \"output_\" + test_date + \"_5.csv\"\n",
        "data_out_24.fillna(\"0\").to_csv(output_csv_file5)"
      ]
    }
  ]
}